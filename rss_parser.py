from concurrent.futures import ProcessPoolExecutor
from logging import config
import re
from urllib.parse import urljoin
import feedparser
import logging
import aiohttp
import hashlib
from typing import Any, Dict, List, Optional, Union, Set, Callable
import asyncio
from defusedxml import ElementTree as ET
from io import BytesIO
from datetime import datetime
from dateutil import parser as date_parser
from bs4 import BeautifulSoup, Tag

logger = logging.getLogger('AsyncRSSParser')

class AsyncRSSParser:
    MAX_ENCLOSURES = 20  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    CONTENT_SELECTORS = [
        'article img',
        '.post-content img',
        '.article-body img',
        'main img',
        'figure img',
        'picture source',
        '[itemprop="image"]',
        '.content img',  # –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è –•–∞–±—Ä–∞
        '.article img',  # –û–±—â–∏–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        '.post__body img',  # –°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –•–∞–±—Ä–∞
        '.story__content img'  # –î–ª—è lenta.ru
    ]

    def __init__(
        self, 
        session: aiohttp.ClientSession, 
        proxy_url: Optional[str] = None, 
        on_session_recreate: Optional[Callable] = None  # –î–æ–±–∞–≤–ª–µ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä
    ):
        self.session = session
        self.proxy_url = proxy_url
        self.controller = None  # –î–æ–±–∞–≤—å—Ç–µ —ç—Ç—É —Å—Ç—Ä–æ–∫—É
        self.on_session_recreate = on_session_recreate  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—Ç—Ä–∏–±—É—Ç–∞
        self.timeout = aiohttp.ClientTimeout(total=30, sock_read=25)
        self.semaphore = asyncio.Semaphore(5)
        self.executor = ProcessPoolExecutor(max_workers=2)
        self.config = config
        self.feed_status = {}
        self.feed_errors = {}
        self.max_retries = 3  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
        self.retry_delay = 0.5  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö

    def set_feed_status(self, url: str, active: bool):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–ª—è RSS-–ª–µ–Ω—Ç—ã"""
        self.feed_status[url] = active

    def refresh_status(self, url: str):
        """–°–±—Ä–æ—Å —Å—á–µ—Ç—á–∏–∫–∞ –æ—à–∏–±–æ–∫ –¥–ª—è –ª–µ–Ω—Ç—ã"""
        if url in self.feed_errors:
            del self.feed_errors[url]
            logger.info(f"RSS status reset for {url}")

    def set_controller(self, controller):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π"""
        self.controller = controller
        
    def set_on_session_recreate(self, callback: Callable):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç callback –¥–ª—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è —Å–µ—Å—Å–∏–∏"""
        self.on_session_recreate = callback

    async def fetch_feed(self, url: str) -> Optional[Dict[str, Any]]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç RSS-–ª–µ–Ω—Ç—É —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        # –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ª–µ–Ω—Ç—ã –≤ –Ω–∞—á–∞–ª–µ –º–µ—Ç–æ–¥–∞
        if not self.feed_status.get(url, True):
            logger.debug(f"–õ–µ–Ω—Ç–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞: {url}")
            return None
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ—Å—Å–∏–∏ –ø–µ—Ä–µ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º –∑–∞–ø—Ä–æ—Å–∞
        if self.session.closed:
            logger.critical("Session is closed! Attempting to recreate...")
            if self.on_session_recreate:
                await self.on_session_recreate()  # –í—ã–∑—ã–≤–∞–µ–º –∫–æ–ª–±—ç–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
            else:
                logger.error("No session recreation callback available!")
                return None

        logger.info(f"Fetching RSS feed: {url}")
        
        for attempt in range(1, self.max_retries + 1):
            try:
                # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Å—Å–∏–∏
                if self.session.closed:
                    logger.error("Session closed unexpectedly during retry")
                    return None
                    
                async with self.session.get(
                    url,
                    proxy=self.proxy_url if self.proxy_url else None,
                    timeout=self.timeout,
                    headers={'User-Agent': 'RSSBot/1.0'}
                ) as response:
                    if response.status != 200:
                        logger.error(f"HTTP error {response.status} for {url}")
                        
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
                        error_msg = (
                            f"‚ö†Ô∏è <b>–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ RSS</b>\n"
                            f"‚îî URL: {url}\n"
                            f"‚îî –ö–æ–¥: {response.status}"
                        )
                        if self.controller:
                            asyncio.create_task(self.controller._send_status_notification(error_msg))
                        
                        return None

                    content = await response.read()
                    logger.debug(f"Raw content received for {url}, length: {len(content)} bytes")
                    
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ
                    success_msg = (
                        f"üì• <b>RSS –∑–∞–≥—Ä—É–∂–µ–Ω</b>\n"
                        f"‚îî URL: {url}\n"
                        f"‚îî –†–∞–∑–º–µ—Ä: {len(content)//1024} KB"
                    )
                    if self.controller:
                        asyncio.create_task(self.controller._send_status_notification(success_msg))
                        
                    return await self._safe_parse_feed(content)
                    
            except aiohttp.ClientOSError as e:
                if "APPLICATION_DATA_AFTER_CLOSE_NOTIFY" in str(e) and attempt < self.max_retries:
                    logger.warning(f"SSL error detected, retrying ({attempt}/{self.max_retries}) for {url}")
                    await asyncio.sleep(self.retry_delay * attempt)
                else:
                    self.feed_errors[url] = self.feed_errors.get(url, 0) + 1
                    logger.error(f"Error fetching {url}: {str(e)}", exc_info=True)
                    
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ —Å–µ—Ç–∏
                    error_msg = (
                        f"‚ö†Ô∏è <b>–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞</b>\n"
                        f"‚îî URL: {url}\n"
                        f"‚îî –û—à–∏–±–∫–∞: {str(e)[:100]}"
                    )
                    if self.controller:
                        asyncio.create_task(self.controller._send_status_notification(error_msg))
                        
                    return None
                    
            except RuntimeError as e:
                if "Session is closed" in str(e):
                    logger.critical("Session closed during request processing")
                    return None
                else:
                    self.feed_errors[url] = self.feed_errors.get(url, 0) + 1
                    logger.error(f"RuntimeError fetching {url}: {str(e)}", exc_info=True)
                    
                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
                    error_msg = (
                        f"‚ö†Ô∏è <b>–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è</b>\n"
                        f"‚îî URL: {url}\n"
                        f"‚îî –û—à–∏–±–∫–∞: {str(e)[:100]}"
                    )
                    if self.controller:
                        asyncio.create_task(self.controller._send_status_notification(error_msg))
                        
                    return None
                    
            except Exception as e:
                self.feed_errors[url] = self.feed_errors.get(url, 0) + 1
                logger.error(f"Error fetching {url}: {str(e)}", exc_info=True)
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ–± –æ–±—â–µ–π –æ—à–∏–±–∫–µ
                error_msg = (
                    f"‚ö†Ô∏è <b>–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞</b>\n"
                    f"‚îî URL: {url}\n"
                    f"‚îî –û—à–∏–±–∫–∞: {str(e)[:100]}"
                )
                if self.controller:
                    asyncio.create_task(self.controller._send_status_notification(error_msg))
                    
                return None
        
        return None
        
    def get_error_count(self, url: str) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ URL"""
        return self.feed_errors.get(url, 0)

    async def _safe_parse_feed(self, xml_content: Any) -> Optional[Dict[str, Any]]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ RSS —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç XXE –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            if xml_content is None:
                return None

            # Try direct feedparser parsing first (faster)
            try:
                parsed = feedparser.parse(xml_content)
                if parsed.get('entries'):
                    return parsed
            except Exception as e:
                logger.debug(f"Direct feedparser parsing failed, trying defusedxml: {str(e)}")

            # Fallback to defusedxml for security
            if isinstance(xml_content, bytes):
                try:
                    xml_content = xml_content.decode('utf-8')
                except UnicodeDecodeError:
                    xml_content = xml_content.decode('latin-1', errors='replace')

            cleaned_content = re.sub(
                r'<!DOCTYPE[^>[]*(\[[^]]*\])?>',
                '',
                xml_content,
                flags=re.IGNORECASE
            )

            try:
                xml_bytes = cleaned_content.encode('utf-8')
                parser = ET.DefusedXMLParser(
                    forbid_dtd=True,
                    forbid_entities=True,
                    forbid_external=True
                )
                tree = ET.parse(BytesIO(xml_bytes), parser=parser)
                root = tree.getroot()
                if root is not None:
                    return feedparser.parse(BytesIO(ET.tostring(root)))
            except Exception as e:
                logger.debug(f"DefusedXML parsing failed, falling back to feedparser: {str(e)}")

            # Final fallback
            return feedparser.parse(cleaned_content)

        except Exception as e:
            logger.error(f"Failed to parse feed content: {str(e)}")
            return None

    def parse_entries(self, feed_content: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ RSS-–ª–µ–Ω—Ç—ã –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–ø–∏—Å–∏"""
        entries = []
        if not feed_content or not isinstance(feed_content, dict) or 'entries' not in feed_content:
            logger.debug("No entries found in feed")
            return entries

        seen_guids: Set[str] = set()

        for entry in feed_content['entries']:
            try:
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏
                guid = self._generate_entry_guid(entry)
                if guid in seen_guids:
                    continue
                seen_guids.add(guid)

                # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è –∑–∞–ø–∏—Å–∏
                link = self._get_entry_link(entry)
                description = self._clean_html(getattr(entry, 'summary', getattr(entry, 'description', '')))

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image_url = self._extract_image_url(entry)
                if not image_url and link:
                    # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ HTML-–æ–ø–∏—Å–∞–Ω–∏—è —Å –±–∞–∑–æ–≤—ã–º URL
                    base_url = link if link else self._get_feed_base_url(feed_content)
                    image_url = self._extract_image_from_html(description, base_url)

                entry_data = {
                    'guid': guid,
                    'title': self._clean_text(getattr(entry, 'title', 'No title')),
                    'description': description,
                    'link': link,
                    'pub_date': self._get_pub_date(entry),
                    'image_url': image_url,
                    'author': self._get_author(entry),
                    'categories': self._get_categories(entry)
                }
                entries.append(entry_data)
            except Exception as e:
                logger.error(f"Error parsing entry: {str(e)}", exc_info=True)
                continue

        logger.debug(f"Parsed {len(entries)} entries from feed")
        return entries

    def _extract_image_from_html(self, html_content: str, base_url: str) -> Optional[str]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ HTML-–∫–æ–Ω—Ç–µ–Ω—Ç–µ"""
        if not html_content:
            return None

        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            candidate_images = []
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º
            selectors = [
                'img',  # –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                'picture source[srcset]',
                '[data-src]',  # Lazy-loaded
                '.post-content img',
                '.article-body img',
                '.content img',
                '.post__body img',  # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –¥–ª—è –•–∞–±—Ä–∞
                '.story__content img'  # Lenta.ru
            ]
            
            for selector in selectors:
                for element in soup.select(selector):
                    src = self._get_image_src(element)
                    if not src:
                        continue
                        
                    normalized_url = self._normalize_image_url(src, base_url)
                    if not normalized_url:
                        continue
                        
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                    if self._is_relevant_image(element, normalized_url):
                        candidate_images.append(normalized_url)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤–æ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            return candidate_images[0] if candidate_images else None
            
        except Exception as e:
            logger.debug(f"HTML content image extraction error: {str(e)}")
            return None

    @staticmethod
    def _get_image_src(element: Tag) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤"""
        for attr in ['src', 'srcset', 'data-src', 'data-lazy-src']:
            if attr in element.attrs:
                value = element[attr]
                if isinstance(value, list):
                    value = value[0]
                return value.split()[0] if ' ' in value else value
        return None

    @staticmethod
    def _is_relevant_image(element: Tag, img_url: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º"""
        # –§–∏–ª—å—Ç—Ä –ø–æ URL
        if any(bad in img_url.lower() for bad in ['pixel', 'icon', 'logo', 'spacer', 'ad', 'button', 'border']):
            return False
            
        # –§–∏–ª—å—Ç—Ä –ø–æ CSS-–∫–ª–∞—Å—Å–∞–º
        classes = element.get('class', [])
        if any(bad in cls.lower() for cls in classes for bad in ['icon', 'logo', 'ad', 'thumb', 'mini']):
            return False
            
        # –§–∏–ª—å—Ç—Ä –ø–æ —Ä–∞–∑–º–µ—Ä—É (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
        width = element.get('width')
        height = element.get('height')
        try:
            if width and height:
                if int(width) < 300 or int(height) < 200:
                    return False
        except ValueError:
            pass
            
        return True

    async def extract_primary_image(self, url: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        for attempt in range(1, self.max_retries + 1):
            try:
                async with self.session.get(
                    url,
                    timeout=self.timeout,
                    headers={'User-Agent': 'RSSBot/1.0'}  # –î–æ–±–∞–≤–ª—è–µ–º User-Agent
                ) as response:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')

                    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ OpenGraph –∏ Twitter Card
                    if meta_image := self._find_meta_image(soup):
                        return meta_image

                    # 2. –ü–æ–∏—Å–∫ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ
                    if content_image := self._find_content_image(soup, url):
                        return content_image

                    # 3. –†–µ–∑–µ—Ä–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
                    return self._find_fallback_image(soup, url)

            except (aiohttp.ClientOSError, asyncio.TimeoutError, aiohttp.ServerDisconnectedError) as e:
                if attempt < self.max_retries:
                    logger.warning(f"Network error detected, retrying ({attempt}/{self.max_retries}) for {url}")
                    await asyncio.sleep(self.retry_delay * attempt)
                else:
                    logger.error(f"Error extracting image from {url}: {str(e)}")
                    return None
                    
            except Exception as e:
                logger.error(f"Error extracting image from {url}: {str(e)}")
                return None
        
        return None
    
    async def extract_all_images(self, url: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –≥–ª—É–±–æ–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            async with self.session.get(url, timeout=self.timeout) as response:
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')

                # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –º–µ—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
                selectors = [
                    'img',                          # –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    'picture source[srcset]',
                    '[data-src]',
                    '.article-content img',
                    '.post-content img',
                    '.content img',
                    '[itemprop="image"]',
                    'figure img',
                    'div[class*="image"] img'
                ]

                images = []
                for selector in selectors:
                    for element in soup.select(selector):
                        img_url = self._get_image_url(element, url)
                        if img_url and img_url not in images:
                            images.append(img_url)
                
                return images

        except Exception as e:
            logger.error(f"Error extracting images from {url}: {str(e)}")
            return []

    def _find_meta_image(self, soup: BeautifulSoup) -> Optional[str]:
        """–ò—â–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –º–µ—Ç–∞-—Ç–µ–≥–∞—Ö"""
        for meta in soup.find_all('meta'):
            if not isinstance(meta, Tag):
                continue
                
            prop = str(meta.get('property', '')).lower()
            name = str(meta.get('name', '')).lower()
            content = str(meta.get('content', ''))

            if any(p in prop for p in ['og:image', 'image']) or \
            any(n in name for n in ['twitter:image']):
                return content if content else None
        return None

    def _find_content_image(self, soup: BeautifulSoup, base_url: str) -> Optional[str]:
        """–ò—â–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –ø–æ –ø–æ–ª–æ–∂–µ–Ω–∏—é –∏ —Ä–∞–∑–º–µ—Ä—É"""
        candidate_images = []
        
        for selector in self.CONTENT_SELECTORS:
            for img in soup.select(selector):
                if not isinstance(img, Tag):
                    continue

                img_src = img.get('src') or img.get('srcset', '')
                img_src = str(img_src).split()[0] if img_src else ''
                
                if img_src and self._is_valid_image(img, img_src):
                    normalized_url = self._normalize_image_url(img_src, base_url)
                    if not normalized_url:
                        continue
                        
                    # –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    relevance = self._image_relevance_score(img, normalized_url)
                    candidate_images.append((relevance, normalized_url))
        
        if not candidate_images:
            return None
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–≤—ã—Å—à–∏–π –±–∞–ª–ª - –ø–µ—Ä–≤—ã–π)
        candidate_images.sort(key=lambda x: x[0], reverse=True)
        return candidate_images[0][1]

    @staticmethod
    def _image_relevance_score(img_tag: Tag, img_url: str) -> int:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –±–∞–ª–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        score = 0
        
        # –ë–æ–Ω—É—Å –∑–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
        if 'data-large-image' in img_tag.attrs:
            score += 50
            
        # –ë–æ–Ω—É—Å –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ URL
        keywords = ['main', 'featured', 'hero', 'cover', 'primary']
        if any(kw in img_url.lower() for kw in keywords):
            score += 30
        
        # –ë–æ–Ω—É—Å –∑–∞ —Ä–∞–∑–º–µ—Ä—ã (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã)
        try:
            width = int(img_tag.get('width', 0))
            height = int(img_tag.get('height', 0))
            area = width * height
            score += min(area // 1000, 40)  # –ú–∞–∫—Å +40 –±–∞–ª–ª–æ–≤ –∑–∞ –±–æ–ª—å—à–∏–µ —Ä–∞–∑–º–µ—Ä—ã
        except:
            pass
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ –∏–∫–æ–Ω–∫–∏
        if 'social' in img_url.lower() or 'icon' in img_url.lower():
            score -= 20
            
        return score

    def _find_fallback_image(self, soup: BeautifulSoup, base_url: str) -> Optional[str]:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        # –õ–æ–≥–æ—Ç–∏–ø —Å–∞–π—Ç–∞
        if logo := soup.find('link', rel=['icon', 'shortcut icon']):
            if isinstance(logo, Tag) and (href := logo.get('href')):
                href = str(href)
                return self._normalize_image_url(href, base_url)

        # –ü–µ—Ä–≤–æ–µ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        for img in soup.find_all('img'):
            if isinstance(img, Tag) and (src := img.get('src')):
                src = str(src)
                if self._is_valid_image(img, src):
                    return self._normalize_image_url(src, base_url)
        return None

    @staticmethod
    def _normalize_image_url(url: Optional[str], base_url: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not url:
            return ""
        
        # –£–±—Ä–∞–ª–∏ str(url) - —Ç–µ–ø–µ—Ä—å url –≤—Å–µ–≥–¥–∞ —Å—Ç—Ä–æ–∫–∞
        if url.startswith(('http://', 'https://')):
            return url
        if url.startswith('//'):
            return f'https:{url}'
        return urljoin(base_url, url)

    @staticmethod
    def _is_valid_image(img_tag: Tag, img_url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not img_url or any(x in img_url.lower() for x in ['pixel', 'icon', 'logo', 'spacer', 'ad']):
            return False

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã –≤ —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        width_str = str(img_tag.get('width', '0'))
        height_str = str(img_tag.get('height', '0'))
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'px')
        width_str = re.sub(r'[^\d]', '', width_str)
        height_str = re.sub(r'[^\d]', '', height_str)
        
        try:
            width_int = int(width_str) if width_str else 0
            height_int = int(height_str) if height_str else 0
            return width_int >= 300 and height_int >= 200
        except ValueError:
            return True

    @staticmethod
    def _get_feed_base_url(feed_content: Any) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –±–∞–∑–æ–≤—ã–π URL –∏–∑ —Ñ–∏–¥–∞"""
        if hasattr(feed_content, 'href'):
            return feed_content.href
        if hasattr(feed_content, 'link'):
            return feed_content.link
        return ''

    @staticmethod
    def _generate_entry_guid(entry: Any) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –∑–∞–ø–∏—Å–∏"""
        if guid := getattr(entry, 'guid', None):
            return str(guid)
        return hashlib.md5(
            f"{entry.get('link','')}"
            f"{entry.get('title','')}"
            f"{entry.get('published','')}"
            f"{entry.get('updated','')}".encode()
        ).hexdigest()

    @staticmethod
    def _clean_text(text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤"""
        if not text:
            return ""
        return re.sub(r'\s+', ' ', text).strip()

    @staticmethod
    def _clean_html(html: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç HTML-—Ç–µ–≥–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not html:
            return ""
        return re.sub(r'<[^>]+>', '', html).strip()

    @staticmethod
    def _get_entry_link(entry: Any) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫—É –∏–∑ –∑–∞–ø–∏—Å–∏"""
        if hasattr(entry, 'link'):
            return entry.link
        return None

    @staticmethod
    def _get_pub_date(entry: Any) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        for attr in ['published', 'updated', 'pubDate', 'date']:
            if hasattr(entry, attr):
                try:
                    return date_parser.parse(str(getattr(entry, attr))).isoformat()
                except Exception:
                    continue
        return datetime.now().isoformat()

    def _extract_image_url(self, entry: Any) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∑–∞–ø–∏—Å–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–µ–¥–∏–∞-–∫–æ–Ω—Ç–µ–Ω—Ç–∞ (Atom) - <media:content>
        if hasattr(entry, 'media_content'):
            for media in entry.media_content[:self.MAX_ENCLOSURES]:
                media_type = getattr(media, 'type', '')
                if media_type.startswith('image/'):
                    url = getattr(media, 'url', None)
                    if url:
                        return str(url)

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–ª–æ–∂–µ–Ω–∏–π (RSS) - <enclosure>
        if hasattr(entry, 'enclosures'):
            for enclosure in entry.enclosures[:self.MAX_ENCLOSURES]:
                enc_type = getattr(enclosure, 'type', '')
                if enc_type.startswith('image/'):
                    url = getattr(enclosure, 'url', getattr(enclosure, 'href', None))
                    if url:
                        return str(url)

        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–∞—Ç—é—Ä (Media RSS) - <media:thumbnail>
        if hasattr(entry, 'media_thumbnail'):
            thumbnails = entry.media_thumbnail
            if not isinstance(thumbnails, list):
                thumbnails = [thumbnails]

            for thumb in thumbnails[:self.MAX_ENCLOSURES]:
                url = getattr(thumb, 'url', None)
                if url:
                    return str(url)

        # 4. –Ø–≤–Ω–æ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –ø–æ–ª—è—Ö
        for field_name in ['image', 'image_url', 'thumbnail']:
            if hasattr(entry, field_name):
                field_value = getattr(entry, field_name)
                if isinstance(field_value, str) and field_value.startswith('http'):
                    return field_value
                elif isinstance(field_value, dict) and 'url' in field_value:
                    return str(field_value['url'])

        # 5. –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        for field in ['media:content', 'media:thumbnail', 'og:image']:
            if field in entry:
                value = entry[field]
                if isinstance(value, dict) and 'url' in value:
                    return str(value['url'])
                elif isinstance(value, list) and len(value) > 0:
                    first_item = value[0]
                    if isinstance(first_item, dict) and 'url' in first_item:
                        return str(first_item['url'])
                    elif isinstance(first_item, str):
                        return first_item
                elif isinstance(value, str):
                    return value

        # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–¥–ª—è —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ç–∏–ø–∞ JSON Feed)
        if hasattr(entry, 'attachments'):
            for attachment in entry.attachments[:self.MAX_ENCLOSURES]:
                if attachment.get('mime_type', '').startswith('image/'):
                    url = attachment.get('url')
                    if url:
                        return str(url)
                    
        # 7. –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ HTML-–∫–æ–Ω—Ç–µ–Ω—Ç–µ (–¥–ª—è Habr –∏ –ø–æ–¥–æ–±–Ω—ã—Ö)
        if hasattr(entry, 'description') and entry.description:
            description = getattr(entry, 'description', '')
            base_url = self._get_feed_base_url(entry) or ''
            image_url = self._extract_image_from_html(description, base_url)
            if image_url:
                return image_url

        return None

    @staticmethod
    def _get_author(entry: Any) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–≤—Ç–æ—Ä–∞ –∑–∞–ø–∏—Å–∏"""
        if hasattr(entry, 'author'):
            return entry.author
        return None

    @staticmethod
    def _get_categories(entry: Any) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–ø–∏—Å–∏"""
        if not hasattr(entry, 'tags'):
            return []

        categories = []
        for tag in entry.tags:
            if hasattr(tag, 'term'):
                categories.append(tag.term)
            elif isinstance(tag, dict) and 'term' in tag:
                categories.append(tag['term'])
            elif isinstance(tag, str):
                categories.append(tag)

        return categories